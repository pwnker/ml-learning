\documentclass{article}

\usepackage{mathtools}
\usepackage{tikz}

\usepackage[T1]{fontenc}
\usepackage{lmodern} % Font Family

\renewcommand{\rmdefault}{lmss}

\begin{document}
\section{Gradient Descent}

\begin{align}
dC(\omega) = \lim_{\varepsilon\to 0}\frac{C(\omega + \varepsilon) - C(\omega)}{\varepsilon} \\
\end{align}

\subsection{Derivitive Of One Parameter Function}
Within the \textit{Twice} example we described a model with one parameter - w 

\def\avgsum[#1, #2]{\frac{1}{#2}\sum_{#1=1}^{#2}}
\begin{align}
    \intertext{The formula had a form like this:}
    f(x) = x \cdot w
\end{align}
\begin{align}
    \intertext{Function $C$ which takes one parameter $w$ is defined as:}
    C(w)  = \avgsum[i, n](x_i \cdot w - y_i)^2 
\end{align}

\begin{align}
    \intertext{Let's compute the derivitive $C'$ of our function:} 
    C'(w) &= \left(C\right)' \\
          &= \left(\avgsum[i, n](x_i \cdot w - y_i)^2\right)' = \\
          &= \left(\avgsum[i, n](x_i \cdot w - y_i)^2 \right)' = \\ 
          &= \frac{1}{n} \left(\sum_{i=1}^{n}(x_i \cdot w - y_i)^2 \right)' = \\ 
          &= \avgsum[i, n] \left( (x_i \cdot w - y_i)^2 \right)' = \\ 
          &= \avgsum[i, n] \left( 2 \cdot (x_i \cdot w - y_i) (x_i \cdot w - y_i)' \right) = \\ 
          &= \avgsum[i, n] \left( 2 \cdot (x_i \cdot w - y_i) \cdot x_i \right) 
\end{align}


\begin{align}
    \intertext{The final form of our derivitive:}
    C'(w) &= \avgsum[i, n] \left( 2 \cdot (x_i \cdot w - y_i) \cdot x_i \right) 
\end{align}

\subsection{One Neuron Model With 2 Inputs}
One neuron model is defined as:  
\begin{align}
    z  = \sigma(x \cdot w_1 + y \cdot w_2 + b) 
\end{align}
\begin{align*}
    \text{$x_1$}  \: &\text{...} \: \text{input parameter}\\
    \text{$x_2$}  \: &\text{...} \: \text{input parameter}\\
    \text{$w_1$}  \: &\text{...} \: \text{weight paramter}\\
    \text{$w_2$}  \: &\text{...} \: \text{weight paramter}\\
    \text{b}  \: &\text{...} \: \text{bias parameter}\\
    \sigma    \: &\text{...} \: \text{sigmoid activation function}
\end{align*}
\begin{center}
\def\d{2}
\begin{tikzpicture}
    \node (x_1) at (-\d,0.75) {$x$}; 
    \node (x_2) at (-\d,-0.75) {$y$}; 
    \node[shape=circle,draw=black] (n) at (0,0) {$\sigma, b^{(1)}$}; 
    \node (y) at (\d,0) {$z$}; 
    \path[->] (x_1) edge node[above] {$w_1$} (n);
    \path[->] (x_2) edge node[above] {$w_2$} (n);
    \path[->] (n) edge (y);
\end{tikzpicture}
\end{center}
\subsubsection{Cost}
Let's recall the Sigmoid activation function  
\begin{align}
    \sigma(x)  &= \frac{1}{1 + e^{-x}} \\ 
    \sigma(x)' &= \sigma(x) \cdot (1 - \sigma(x)) 
\end{align}
Let's define the cost function $C$ for our model
\begin{align}
    a_i  &= \sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) \\ 
    \Aboxed{C(x) &= \avgsum[i, n](a_i - z_i)^2}
\end{align}
Let's compute the derivitive $C'$ for our function\\
\\
We have to modify TWO parameters $w, b$\\
\\
For this we will use PARTIAL DERIVITIVES this means that first we compute a derivitive in respect to $w_1$, $w_2$ and then we compute another derivitive in respect to $b$ \\
\def\pd[#1]{\partial_{#1}}
\begin{enumerate}
    \item{
        Partial Derivitive in respect to $w_1$
        \begin{align}
            a_i  &= \sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) = \\ 
            \pd[w_1]a_i  &= \pd[w_1](\sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) = \\ 
                         &= a_i(1 - a_i) \pd[w_1](x_i \cdot w_1 + y_i \cdot w_2 +b) = \\ 
            \pd[w_1]a_i  &= a_i(1 - a_i) \cdot x_i \\ 
            \\
            \pd[w_1]C &=\pd[w_1]\left(\avgsum[i, n](a_i - z_i)^2\right) =  \\ 
                    &=\avgsum[i, n]\pd[w_1]\left((a_i - z_i)^2\right) =  \\ 
                    &=\avgsum[i, n]2 \cdot (a_i - z_i)\pd[w_1]\left(a_i - z_i\right) = \\ 
                    &=\avgsum[i, n]2 \cdot (a_i - z_i)\pd[w_1]a_i = \\ 
            \pd[w_1]C &=\avgsum[i, n]2 \cdot (a_i - z_i) \cdot a_i(1 - a_i) \cdot x_i \\ 
        \end{align}
    }
    \item{
        Partial Derivitive in respect to $w_2$
        \begin{align}
            a_i  &= \sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) = \\ 
            \pd[w_2]a_i  &= \pd[w_2](\sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) = \\ 
                         &= a_i(1 - a_i) \pd[w_2](x_i \cdot w_1 + y_i \cdot w_2 +b) = \\ 
            \pd[w_2]a_i  &= a_i(1 - a_i) \cdot y_i \\ 
            \\
            \pd[w_2]C &=\pd[w_2]\left(\avgsum[i, n](a_i - z_i)^2\right) =  \\ 
                    &=\avgsum[i, n]\pd[w_2]\left((a_i - z_i)^2\right) =  \\ 
                    &=\avgsum[i, n]2 \cdot (a_i - z_i)\pd[w_2]\left(a_i - z_i\right) = \\ 
                    &=\avgsum[i, n]2 \cdot (a_i - z_i)\pd[w_2]a_i = \\ 
            \pd[w_2]C &=\avgsum[i, n]2 \cdot (a_i - z_i) \cdot a_i(1 - a_i) \cdot y_i \\ 
        \end{align}
    }
    \item{
        Partial Derivitive in respect to $b$
        \begin{align}
            a_i  &= \sigma(x_i \cdot w_1 + y_i \cdot w_2 + b)  \\ 
            \pd[b]a_i &= \pd[b](\sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) = \\ 
                      &= a_i(1 - a_i)\pd[b](x_i \cdot w_1 + y_i \cdot w_2 +b) = \\ 
                      &= a_i(1 - a_i) \cdot 1 = \\ 
            \pd[b]a_i &= a_i(1 - a_i) \\ 
            \\
            \pd[b]C &=\pd[b]\left(\avgsum[i, n](a_i - z_i)^2\right) =  \\ 
                    &=\avgsum[i, n]\pd[b]\left((a_i - z_i)^2\right) =  \\ 
                    &=\avgsum[i, n]2 \cdot (a_i - z_i)\pd[b]\left(a_i - z_i\right) = \\ 
                    &=\avgsum[i, n]2 \cdot (a_i - z_i)\pd[b]a_i = \\ 
            \pd[b]C &=\avgsum[i, n]2 \cdot (a_i - z_i) \cdot a_i(1 - a_i) \\ 
        \end{align}
    }
\end{enumerate}
To summarize the partial derivitives are:
\begin{align}
    a_i  &= \sigma(x_i \cdot w_1 + y_i \cdot w_2 + b) \\ 
    \pd[w_1]C &=\avgsum[i, n]2 \cdot (a_i - z_i) \cdot a_i(1 - a_i) \cdot x_i \\ 
    \pd[w_2]C &=\avgsum[i, n]2 \cdot (a_i - z_i) \cdot a_i(1 - a_i) \cdot y_i \\ 
    \pd[b]C   &=\avgsum[i, n]2 \cdot (a_i - z_i) \cdot a_i(1 - a_i) \\ 
\end{align}
\newpage
\subsubsection{Execution Time Comparison}
Let's compare computation time differance between \textbf{Finite Difference} and \textbf{Gradient Descent}\\
My machine is Lenovo Legion Slim 5:
\begin{itemize}
    \item {All computations are run on the CPU}
    \item {CPU: AMD Rayzen 7 7840HS (16) 5.137Ghz}
\end{itemize}
The test:
\begin{itemize}
    \item {Neural network will try to learn the proper configuration for simulating NAND gate} 
    \item {Comparison of training the model using the \textit{Finite Difference} method and \textit{Gradient Descent}} 
    \item {8.000.000 iterations(epochs) of training will be run (overkill I know)} 
\end{itemize}
\textbf{RESULTS:}\\
\indent \[ \textbf{Finite Difference}:\ \approx 1.556\ seconds \]
\indent \[ \textbf{Gradient Descent}:\ \approx 0.473\ seconds \]
Let's not forget that NAND gate simulation is preatty much trivial and both methods of computation would have approximatly the same time when not doing as much iterations(epochs) of training \\
\newpage
\def\arg[#1, #2]{#1^{(#2)}}
\subsection{Two Neuron Model And 1 Input}
\begin{center}
\def\d{2}
\begin{tikzpicture}
    \node (x_1) at (-\d, 0) {$a^{(0)}$}; 
    \node[shape=circle,draw=black] (n1) at (0,0) {$\sigma,  b^{(1)}$}; 
    \node[shape=circle,draw=black] (n2) at (\d,0) {$\sigma, \arg[b, 2]$}; 
    \node (y) at ({2*\d},0) {$z$}; 
    \path[->] (x_1) edge node[above] {$w^{(1)}$} (n1);
    \path[->] (n1) edge node[above] {$\arg[w, 2]$} (n2);
    \path[->] (n2) edge (y);
\end{tikzpicture}
\end{center}
Let's define the mathematical model
\begin{align}
    a^{(1)} &= \sigma(x \cdot w^{(1)} + b^{(1)})\\
    a^{(2)} &= \sigma(a^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
    \\
    \text{$a^{(i)}$}  \: &\text{...} \: \text{activation of the i-th layer}\\
    \\
    z &= a^{(2)}\\
    z &= \sigma(a^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
\end{align}
\subsection{Cost}
\subsubsection{Cost Of The 2nd Layer}
Let's define the cost function $C^{(2)}$ for the second layer of our model:
\begin{align}
    \arg[a_i, 1] &= \sigma(x_i \cdot w^{(1)} + b^{(1)})\\
    \arg[a_i, 2] &= \sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
    \\
    \text{$a_i^{(l)}$}  \: &\text{...} \: \text{activation of the i-th sample of the l-th layer}
    \\
    \\
    \Aboxed{C^{(2)}(x) &= \avgsum[i, n](a_i^{(2)} - z_i)^2} 
\end{align}
Before we start computing derivitives let's think about them \\
\begin{itemize}
    \item {Firstly we have to compute partial derivitives for $\arg[w, 2]$ nad $\arg[b, 2]$ which should not be hard becuase we already converd similar calculations in the past}
    \item {When we try to compute partial derivitives inner $\arg[w, 1]$ and $\arg[b, 1]$ we notice that the parameters $\arg[w, 1]$ and $\arg[b, 1]$ are deeply nested inside $\arg[a, 1]$ which can present a challange when trying to compute partial derivates}
    \item {Introduce a separate cost functions for each indivudual layer\\
                                                        - Alexey Kutepov}
    \item {For each of these \textit{specialized} layers you compute the cost only for the variables that are nearly accessable}
    \item {Let's treat the \textit{previous activation} as a variable of the cost function and let's differentiate it}
    \item {The result of the differentiation of the cost function is actually an \textit{differance(error)} that we can use for the computation of the \textit{Difference(error)} of the inner layer}
    \item {We continue to compute these \textit{differances(errors} for all layers all the way back to the \textit{input layer} => This is where the idea of \textbf{back-propagation} comes to play}
\end{itemize}
Let's compute the derivitive $C^{(2)}$$'$ of our function\\
\\
\indent Partial derivitive in regards to $\arg[w, 2]$
\begin{align}
    a_i^{(2)} &= \sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
    \pd[{\arg[w, 2]}]a_i^{(2)} 
        &= \pd[{\arg[w, 2]}]\sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
        &= a_i^{(1)}(1 - a_i^{(1)}) \cdot \pd[{\arg[w, 2]}](a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
    \pd[{\arg[w, 2]}]a_i^{(2)} &= a_i^{(1)}(1 - a_i^{(1)}) \cdot a_i^{(1)}\\
    \\
    \\
    \pd[{\arg[w, 2]}]C^{(2)} 
                &=  \pd[{\arg[w, 2]}]\left(\avgsum[i, n](a_i^{(2)} - z_i)^2\right)    \\
                &=  \avgsum[i, n]\pd[{\arg[w, 2]}](a_i^{(2)} - z_i)^2)    \\
                &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot \pd[{\arg[w, 2]}](a_i^{(2)} - z_i)    \\
                &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot \pd[{\arg[w, 2]}]a_i^{(2)}    \\
    \pd[{\arg[w, 2]}]C^{(2)}  &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot a_i^{(1)}(1 - a_i^{(1)}) \cdot a_i^{(1)}    \\
\end{align}
\indent Partial derivitive in regards to $\arg[b, 2]$
\begin{align}
    a_i^{(2)} &= \sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
    \pd[{\arg[b, 2]}]a_i^{(2)} 
        &= \pd[{\arg[b, 2]}]\sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
        &= a_i^{(1)}(1 - a_i^{(1)}) \cdot \pd[{\arg[b, 2]}](a_i^{(1)} \cdot \arg[w, 2] + {\arg[b, 2]})\\
    \pd[{\arg[b, 2]}]a_i^{(2)}   &= a_i^{(1)}(1 - a_i^{(1)}) \\
    \\
    \\
    \pd[{\arg[b, 2]}]C^{(2)}
                &=  \pd[{\arg[b, 2]}]\left(\avgsum[i, n](a_i^{(2)} - z_i)^2\right)    \\
                &=  \avgsum[i, n]\pd[{\arg[b, 2]}](a_i^{(2)} - z_i)^2)    \\
                &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot \pd[{\arg[b, 2]}](a_i^{(2)} - z_i)    \\
                &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot \pd[{\arg[b, 2]}]a_i^{(2)}    \\
    \pd[{\arg[b, 2]}]C^{(2)} &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot a_i^{(1)}(1 - a_i^{(1)})    \\
\end{align}
\indent Partial derivitive in regards to $\arg[a_i, 1]$
\begin{align}
    a_i^{(2)} &= \sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
    \pd[{\arg[a_i, 1]}]a_i^{(2)} 
        &= \pd[{\arg[a_i, 1]}]\sigma(a_i^{(1)} \cdot \arg[w, 2] + \arg[b, 2])\\
        &= a_i^{(1)}(1 - a_i^{(1)}) \cdot \pd[{\arg[a_i, 1]}](a_i^{(1)} \cdot \arg[w, 2] + {\arg[b, 2]})\\
    \pd[{\arg[a_i, 1]}]a_i^{(2)} &= a_i^{(1)}(1 - a_i^{(1)}) \cdot \arg[w, 2]\\
    \\
    \\
    \pd[{\arg[a_i, 1]}]C^{(2)}
                &=  \pd[{\arg[a_i, 1]}]\left(\avgsum[i, n](a_i^{(2)} - z_i)^2\right)    \\
                &=  \avgsum[i, n]\pd[{\arg[a_i, 1]}](a_i^{(2)} - z_i)^2)    \\
                &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot \pd[{\arg[a_i, 1]}](a_i^{(2)} - z_i)    \\
                &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot \pd[{\arg[a_i, 1]}]a_i^{(2)}    \\
    \pd[{\arg[a_i, 1]}]C^{(2)} &=  \avgsum[i, n]2 \cdot (a_i^{(2)} - z_i) \cdot a_i^{(1)}(1 - a_i^{(1)}) \cdot \arg[w, 2]    \\
\end{align}
\newpage
\subsubsection{Cost Of The 1st layer}
Here is en example of \textbf{back-propagation} in the works:
\begin{itemize}
    \item {We know that the expected value for the 2nd layer is $z_i$} 
    \item {Let's use $e_i$ to donote the expacted value for i-th sample of the 1st layer} 
    \item {$e_i$} is the differance between the activation of the first layer and the derivitive of the cost function of the second layer
    $$e_i = \arg[a_i, 1] - \pd[{\arg[a_i, 1]}]C^{(2)}$$ \\
    \item{Now we can define the cost function of the 1st layer as such}
    $$\arg[C, 1] = \avgsum[i, n] (\arg[a_i, 1] - e_i)^2$$ \\
\end{itemize}
\newpage 
\noindent Now that we can easily accessi whe weights and bias of the first layer let's compute the corresponding partial derivitives
\begin{align}
    \arg[a_i, 1] &= \sigma(x_i \cdot w^{(1)} + b^{(1)})\\
    \\
    \pd[{\arg[w, 1]}]\arg[a_i, 1] 
                 &= \arg[a_i, 1](1 - \arg[a_i, 1]) \cdot \arg[a_i, 0] \\ 
    \pd[{\arg[b, 1]}]\arg[a_i, 1] 
                 &= \arg[a_i, 1](1 - \arg[a_i, 1])  \\ 
    \intertext{We know that the activation of 0th layer - $\arg[a_i, 0]$ represnets the i-th input value - $x_i$}
    \Aboxed{\pd[{\arg[w, 1]}]\arg[a_i, 1] =\arg[a_i, 1](1 - \arg[a_i, 1]) \cdot x_i}
    \Aboxed{\pd[{\arg[b, 1]}]\arg[a_i, 1] = \arg[a_i, 1](1 - \arg[a_i, 1])}   
\end{align}
\begin{align}
    \pd[{\arg[w, 1]}]\arg[C, 1] 
                 &= \pd[{\arg[w, 1]}] \left(\avgsum[i, n] (\arg[a_i, 1] - e_i)^2\right) = \\
                 &= \avgsum[i, n] \pd[{\arg[w, 1]}](\arg[a_i, 1] - e_i)^2 = \\
                 &= \avgsum[i, n] 3 \cdot (\arg[a_i, 1] - e_i) \cdot \pd[{\arg[w, 1]}](\arg[a_i, 1] - e_i) = \\
                 &= \avgsum[i, n] 2 \cdot (\arg[a_i, 1] - e_i) \cdot \pd[{\arg[w, 1]}]\arg[a_i, 1] = \\
                 &= \avgsum[i, n] 2 \cdot (\arg[a_i, 1] - (\arg[a_i, 1] - \pd[{\arg[a_i, 1]}]C^{(2)})) \cdot \pd[{\arg[w, 1]}]\arg[a_i, 1] = \\
                 &= \avgsum[i, n] 2 \cdot (\arg[a_i, 1] - \arg[a_i, 1] + \pd[{\arg[a_i, 1]}]C^{(2)}) \cdot \pd[{\arg[w, 1]}]\arg[a_i, 1] = \\
                 &= \avgsum[i, n] 2 \cdot \pd[{\arg[a_i, 1]}]C^{(2)} \cdot \pd[{\arg[w, 1]}]\arg[a_i, 1] = \\
                 &= \avgsum[i, n] 2 \cdot \pd[{\arg[a_i, 1]}]C^{(2)} \cdot \arg[a_i, 1](1 - \arg[a_i, 1]) \cdot x_i = \\
                 &= \avgsum[i, n] 2 \cdot \pd[{\arg[a_i, 1]}]C^{(2)} \cdot \arg[a_i, 1](1 - \arg[a_i, 1]) \cdot x_i \\
    \\
    \\
    \pd[{\arg[b, 1]}]\arg[C, 1] &= \avgsum[i, n] 2 \cdot \pd[{\arg[a_i, 1]}]C^{(2)} \cdot \arg[a_i, 1](1 - \arg[a_i, 1])
\end{align}

\newpage

\subsection{Arbitrary Neural Model With 1 Input}

Let's say that our neuarl model has $m$ layers \\

\subsubsection{Feed-Forward}
TODO: Describe \\

To calculate the activation of the $l$-th layer for the $i$-th sample you must multiply the activation of the $l-1$ layer with the weight of the $l$-th layer then add the bias of the $l$-th layer and apply an activation function to the result. \\
\\
\textit{As an example the sigmoid}-$\sigma$ \textit{function is used.} \\
\\
Let's also consider the activation of the 0th layer $\arg[a_i, 0]$ to represent the $i$-th sample of the input data - $x_i$

\begin{align}
    \arg[a_i, l] &= \sigma(\arg[a_i, l-1] \cdot \arg[w, l] + \arg[b, l])
    \intertext{Partial derivitives}
    \pd[{\arg[w, l]}]\arg[a_i, l]     &= \arg[a_i, l] \cdot (1 - \arg[a_i, l]) \cdot \arg[a_i, l-1] \\
    \pd[{\arg[b, l]}]\arg[a_i, l]     &= \arg[a_i, l] \cdot (1 - \arg[a_i, l])                      \\ 
    \pd[{\arg[a_i, l-1]}]\arg[a_i, l] &= \arg[a_i, l] \cdot (1 - \arg[a_i, l]) \cdot \arg[w, l] 
\end{align}

\subsubsection{Back-Propaggation}
TODO: Describe \\
Let's denote this differance $\arg[a_i, m] - z_i$ as a parial derivitive $\pd[{\arg[a_i, m+1]}]$

\begin{align}
    \Aboxed{\arg[C, l]                        &= \avgsum[i, n] (\pd[{\arg[a_l, l]}]\arg[C, l+0])^2}                                                           \\
    \\
    \pd[{\arg[w, l]}]\arg[C, l]       &= \avgsum[i, n] 2 \cdot (\pd[{\arg[a_l, l]}]\arg[C, l+1]) \cdot \arg[a_i, l] \cdot (1 - \arg[a_i, l]) \cdot \arg[a_i, l-1] \\
    \pd[{\arg[b, l]}]\arg[C, l]       &= \avgsum[i, n] 2 \cdot (\pd[{\arg[a_l, l]}]\arg[C, l+1]) \cdot \arg[a_i, l] \cdot (1 - \arg[a_i, l])           \\
    \pd[{\arg[a_i, l - 1]}]\arg[C, l] &= \avgsum[i, n] 2 \cdot (\pd[{\arg[a_l, l]}]\arg[C, l+1]) \cdot \arg[a_i, l] \cdot (1 - \arg[a_i, l]) \cdot \arg[w, l]       
\end{align}

\subsection{Combining Feed-Forward and Back-Propaggation}
Let's describe the way we will use conecpts descrabed in the previous two sections togeter. \\
\\
Let's say we have $k$ samples of trainig data \\
\\
1. For each smaple we will \textbf{forward} the sample through the neural network until we reach the output. - \textbf{Feed-Forward} \\
\textit{Node: Neural Network output is the} $\arg[a_i, m+1]$ \textit{activation} \\
\\
2. Straing from the $m+1$ activation let's move backwards through the neual network propagating the differances of layer activations - \textbf{Back-Propaggation} 

\end{document}

